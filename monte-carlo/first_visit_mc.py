import numpy as np
from typing import Sequence


class LinearWorld:
    """A 1D world where the agent can only go left or right

    Parameters
    ----------
        number_of_fields : int
            The linear world consists of fields, each field has a state value
    
    Attributes
    ----------
        states : array-like[float]
            There exists only one terminal state, the most left field, every other
            state gets assigned a random state value, sampled from a uniform distribution
        start_position : int
            The position where the agent starts every time a new episode is triggered
        current_position : int
            The position of the agent
    
    Methods
    -------
        reset()
            The agent will start again from its start position
    """


    def __init__(self, number_of_fields: int, start_position: int):
        self.number_of_fields = number_of_fields
        self.states           = np.random.rand(number_of_fields)
        self.states[0]        = 0
        self.start_position   = start_position
        self.current_position = start_position


    def reset(self) -> None:
        """The current position of the agent is reset to the start position"""
        self.current_position = self.start_position


class Policy:
    """Defines the policy which is used for estimating the value of the Linear World
    
    Parameters
    ----------
        policy : array-like[int]
            Policy which the agent should follow in the Linear World
    """


    def __init__(self, policy: Sequence[int]):
        self.policy = policy


    def __getitem__(self, ix):

        return self.policy[ix]


class FirstVisitMC:
    """Implements the first visit MC algorithm for estimating the value function
    
    Parameters
    ----------
        world : LinearWorld
            A Linear world which defines the state values for every field
        policy : Policy
            The agent will follow this policy, the policy has only two available possible actions,
            going left which is represented by `0` and going right which is represented by `1`

    Methods
    -------
        estimate_value(max_iterations) 2d-array[float]
            Implementation of the algorithm which will estimate the value function for the given policy
    """
    def __init__(self, world: LinearWorld, policy: Policy):
        self.world  = world
        self.policy = policy


    def estimate_value(self, max_iterations: int) -> Sequence[float]:
        """The first visit MC algorithm will compute the expected return
        when visiting a state s for the first time
        
        Parameters
        ----------
            max_iterations : int
                After the maximum number of iterations is reached, the estimation will stop if 
                it was not stopped beforehand
        """
        returns = {state: [0.0] for state in range(self.world.number_of_fields)}
        iteration = 0
        while iteration < max_iterations:
            self.world.reset()
            episode = self.__generate_episode()
            visited = {}
            index   = 0
            for state, reward in episode:
                if state not in visited:
                    visited[state] = "visited"
                    G = reward
                    for _, future_reward in episode[index+1:]:
                        if len(episode[index+1:]) != 0:
                            G += future_reward
                    returns[state].append(G)
                    self.world.states[state] = np.mean(returns[state])
                index += 1
            for state in returns.keys():
                if state not in visited:
                    self.world.states[state] = -np.inf
            iteration += 1

        return self.world.states


    def __generate_episode(self) -> list[list[int]]:
        """An episode is generated by following the given policy, the episode
        is needed for the estimation of the value function

        Returns
        -------
            2d-array[int]
                An episode consists of the states which are reached by following the
                given policy and the rewards which one obtains for every state
        """
        episode = []
        while True:
            current_position = self.world.current_position
            if current_position == 0:
                episode.append([current_position, 0])
                break
            episode.append([current_position, -1])
            # going left action
            if self.policy[current_position] == 0:
                self.world.current_position -= 1
                continue
            # going right action
            if self.policy[current_position] == 1:
                if current_position == self.world.number_of_fields - 1:
                    break
                self.world.current_position += 1
            
        return episode


def main():
    WORLDLENGTH    = 10
    START_POSITION = 4
    POLICY         = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]
    linear_world   = LinearWorld(number_of_fields = WORLDLENGTH, start_position = START_POSITION)
    policy         = Policy(policy = POLICY)
    first_visit_mc = FirstVisitMC(world = linear_world, policy = policy)
    value          = first_visit_mc.estimate_value(max_iterations = 1000)
    print(f"Esimated value for the Linear World:\n{value}")


if __name__ == "__main__":
    main()